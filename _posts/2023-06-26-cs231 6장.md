---
layout : single-2
title : "cs 231 6ì¥"
description : "cs231 6ì¥ ì •ë¦¬ ì§„í–‰ì¤‘"
categories : cs231
tag : [python, AI]
toc : true
toc_sticky : true
author_profile : false
use_math : true

---

<h1>cs231 6ì¥ ì •ë¦¬</h1> <br>

# 1. Quick intro without brain analogies
It is possible to introduce neural networks without appealing to brain analogies. In the section on linear classification we computed scores for different visual categories given the image using the formula s=Wx
, where W was a matrix and x was an input column vector containing all pixel data of the image. In the case of CIFAR-10, x is a [3072x1] column vector, and W is a [10x3072] matrix, so that the output scores is a vector of 10 class scores.

í•´ì„
- í° ìƒê°ì—†ì´ ì‹ ê²½ë§ì„ ë„ì…í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì„ í˜• ë¶„ë¥˜ ì„¹ì…˜ì—ì„œ ê³µì‹ s=Wxë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ê°€ ì£¼ì–´ì§„ ë‹¤ì–‘í•œ ì‹œê°ì  ë²”ì£¼ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ê³„ì‚°í• ê²ƒì´ë‹¤.
ì—¬ê¸°ì„œ WëŠ” í–‰ë ¬ì´ê³  xëŠ” ì´ë¯¸ì§€ì˜ ëª¨ë“  í”½ì…€ ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ì…ë ¥ ì—´ ë²¡í„°ì…ë‹ˆë‹¤. CIFAR-10ì˜ ê²½ìš° xëŠ” [3072x1] ì—´ ë²¡í„°ì´ê³  WëŠ” [10x3072] í–‰ë ¬ì´ë¯€ë¡œ ì¶œë ¥ ì ìˆ˜ëŠ” í´ë˜ìŠ¤ ì ìˆ˜ 10ê°œì˜ ë²¡í„°ê°€ ë©ë‹ˆë‹¤.

An example neural network would instead compute s=W2max(0,W1x). Here, W1 could be, for example, a [100x3072] matrix transforming the image into a 100-dimensional intermediate vector. The function max(0,âˆ’) is a non-linearity that is applied elementwise. There are several choices we could make for the non-linearity (which weâ€™ll study below), but this one is a common choice and simply thresholds all activations that are below zero to zero. Finally, the matrix W2 would then be of size [10x100], so that we again get 10 numbers out that we interpret as the class scores. Notice that the non-linearity is critical computationally - if we left it out, the two matrices could be collapsed to a single matrix, and therefore the predicted class scores would again be a linear function of the input. The non-linearity is where we get the wiggle. The parameters W2,W1 are learned with stochastic gradient descent, and their gradients are derived with chain rule (and computed with backpropagation).

í•´ì„
- ì˜ˆì œ ì‹ ê²½ë§ì€ ëŒ€ì‹  s=W2max(0,W1x)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ W1ì€ ì˜ˆë¥¼ ë“¤ì–´ ì´ë¯¸ì§€ë¥¼ 100ì°¨ì› ì¤‘ê°„ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” [100x3072] í–‰ë ¬ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- í•¨ìˆ˜ max(0,-)ëŠ” ìš”ì†Œë³„ë¡œ ì ìš©ë˜ëŠ” ë¹„ì„ í˜•ì„±ì…ë‹ˆë‹¤(í™œì„±í™” í•¨ìˆ˜). ë¹„ì„ í˜•ì„±ì— ëŒ€í•´ ìš°ë¦¬ê°€ í•  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ì„ íƒì´ ìˆì§€ë§Œ(relu, tanh, etc) ì´ê²ƒì€ ì¼ë°˜ì ì¸ ì„ íƒì´ë©° ë‹¨ìˆœíˆ 0ì—ì„œ 0 ë¯¸ë§Œì¸ ëª¨ë“  í™œì„±í™” ì„ê³„ê°’ì„ ì§€ì •í•©ë‹ˆë‹¤.
- ë§ˆì§€ë§‰ìœ¼ë¡œ í–‰ë ¬ W2ì˜ í¬ê¸°ëŠ” [10x100]ì´ë¯€ë¡œ í´ë˜ìŠ¤ ì ìˆ˜ë¡œ í•´ì„í•˜ëŠ” 10ê°œì˜ ìˆ«ìë¥¼ ë‹¤ì‹œ ì–»ìŠµë‹ˆë‹¤. ë¹„ì„ í˜•ì„±ì€ ê³„ì‚°ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë¹„ì„ í˜•ì„±ì„ ìƒëµí•˜ë©´ ë‘ í–‰ë ¬ì´ ë‹¨ì¼ í–‰ë ¬ë¡œ ì¶•ì†Œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ì ìˆ˜ëŠ” ë‹¤ì‹œ ì…ë ¥ì˜ ì„ í˜• í•¨ìˆ˜ê°€ ë©ë‹ˆë‹¤.
- ë¹„ì„ í˜•ì„±ì€ ìš°ë¦¬ê°€ í”ë“¤ë¦¼ì„ ì–»ëŠ” ê³³ì…ë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ W2, W1ì€ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ í•™ìŠµë˜ê³  í•´ë‹¹ ê²½ì‚¬ë„ëŠ” ì²´ì¸ ê·œì¹™ìœ¼ë¡œ ë„ì¶œë©ë‹ˆë‹¤(ì—­ì „íŒŒë¡œ ê³„ì‚°ë¨).

A three-layer neural network could analogously look like s=W3max(0,W2max(0,W1x)), where all of W3,W2,W1 are parameters to be learned.
The sizes of the intermediate hidden vectors are hyperparameters of the network and weâ€™ll see how we can set them later. 
Lets now look into how we can interpret these computations from the neuron/network perspective.

í•´ì„
- 3ê°œì˜ ê³„ì¸µ ì‹ ê²½ë§ì€ ìœ ì‚¬í•˜ê²Œ s=W3max(0,W2max(0,W1x))ì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ëª¨ë“  W3,W2,W1ì€ í•™ìŠµí•  ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤. 
- ì¤‘ê°„ íˆë“  ë²¡í„°ì˜ í¬ê¸°ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì´ë©° ë‚˜ì¤‘ì— ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. 
- ì´ì œ ë‰´ëŸ°/ë„¤íŠ¸ì›Œí¬ ê´€ì ì—ì„œ ì´ëŸ¬í•œ ê³„ì‚°ì„ í•´ì„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

# 2. Modeling one neuron
The area of Neural Networks has originally been primarily inspired by the goal of modeling biological neural systems,
but has since diverged and become a matter of engineering and achieving good results in Machine Learning tasks. 
Nonetheless, we begin our discussion with a very brief and high-level description of the biological system that a large portion of this area has been inspired by.

- ì‹ ê²½ë§ ì˜ì—­ì€ ì›ë˜ ìƒë¬¼í•™ì  ì‹ ê²½ ì‹œìŠ¤í…œì„ ëª¨ë¸ë§í•˜ëŠ” ëª©í‘œì—ì„œ ì˜ê°ì„ ë°›ì•˜ì§€ë§Œ, ì´í›„ ê¸°ê³„ í•™ìŠµ ì‘ì—…ì—ì„œ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ê³  ì—”ì§€ë‹ˆì–´ë§ì˜ ë¬¸ì œê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.
- ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ìš°ë¦¬ëŠ” ì´ ë¶„ì•¼ì˜ ë§ì€ ë¶€ë¶„ì´ ì˜ê°ì„ ë°›ì€ ìƒë¬¼í•™ì  ì‹œìŠ¤í…œì— ëŒ€í•œ ë§¤ìš° ê°„ëµí•˜ê³  ë†’ì€ ìˆ˜ì¤€ì˜ ì„¤ëª…ìœ¼ë¡œ ë…¼ì˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.

## 2.1 Biological motivation and connections

The basic computational unit of the brain is a neuron. Approximately 86 billion neurons can be found in the human nervous system and they are connected with approximately 10^14 - 10^15 synapses.<br> 
The diagram below shows a cartoon drawing of a biological neuron (left) and a common mathematical model (right)<br>
Each neuron receives input signals from its dendrites and produces output signals along its (single) axon.<br>
The axon eventually branches out and connects via synapses to dendrites of other neurons. <br>
In the computational model of a neuron, the signals that travel along the axons (e.g. x0) interact multiplicatively (e.g. w0x0) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. w0).<br>
The idea is that the synaptic strengths (the weights w) are learnable and control the strength of influence (and its direction: excitory (positive weight) or inhibitory (negative weight)) of one neuron on another.<br>
In the basic model, the dendrites carry the signal to the cell body where they all get summed.<br>
If the final sum is above a certain threshold, the neuron can fire, sending a spike along its axon.<br>
In the computational model, we assume that the precise timings of the spikes do not matter, and that only the frequency of the firing communicates information.<br> 
Based on this rate code interpretation, we model the firing rate of the neuron with an activation function f, which represents the frequency of the spikes along the axon.<br> 
Historically, a common choice of activation function is the sigmoid function Ïƒ, since it takes a real-valued input (the signal strength after the sum) and squashes it to range between 0 and 1. We will see details of these activation functions later in this section.<br>

í•´ì„
- ë‡Œì˜ ê¸°ë³¸ ê³„ì‚° ë‹¨ìœ„ëŠ” ë‰´ëŸ°ì…ë‹ˆë‹¤. ì¸ê°„ì˜ ì‹ ê²½ê³„ì—ëŠ” ì•½ 860ì–µ ê°œì˜ ë‰´ëŸ°ì´ ìˆìœ¼ë©° ì•½ 10^14 - 10^15ê°œì˜ ì‹œëƒ…ìŠ¤ì™€ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- ì•„ë˜ ë‹¤ì´ì–´ê·¸ë¨ì€ ìƒë¬¼í•™ì  ë‰´ëŸ°(ì™¼ìª½)ê³¼ ì¼ë°˜ì ì¸ ìˆ˜í•™ì  ëª¨ë¸(ì˜¤ë¥¸ìª½)ì˜ ê·¸ë¦¼ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- ê° ë‰´ëŸ°ì€ ìˆ˜ìƒëŒê¸°ë¡œë¶€í„° ì…ë ¥ ì‹ í˜¸ë¥¼ ë°›ê³  (ë‹¨ì¼) ì¶•ì‚­ì„ ë”°ë¼ ì¶œë ¥ ì‹ í˜¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ì¶•ì‚­ì€ ê²°êµ­ ë¶„ê¸°ë˜ì–´ ì‹œëƒ…ìŠ¤ë¥¼ í†µí•´ ë‹¤ë¥¸ ë‰´ëŸ°ì˜ ìˆ˜ìƒëŒê¸°ì— ì—°ê²°ë©ë‹ˆë‹¤.
- ë‰´ëŸ°ì˜ ê³„ì‚° ëª¨ë¸ì—ì„œ ì¶•ì‚­(ì˜ˆ: x0)ì„ ë”°ë¼ ì´ë™í•˜ëŠ” ì‹ í˜¸ëŠ” í•´ë‹¹ ì‹œëƒ…ìŠ¤(ì˜ˆ: w0)ì˜ ì‹œëƒ…ìŠ¤ ê°•ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë¥¸ ë‰´ëŸ°ì˜ ìˆ˜ìƒëŒê¸°ì™€ ìŠ¹ì‚°ì ìœ¼ë¡œ(ì˜ˆ: w0x0) ìƒí˜¸ ì‘ìš©í•©ë‹ˆë‹¤.
- ì•„ì´ë””ì–´ëŠ” ì‹œëƒ…ìŠ¤ ê°•ë„(ê°€ì¤‘ì¹˜ w)ê°€ í•™ìŠµ ê°€ëŠ¥í•˜ê³  ë‹¤ë¥¸ ë‰´ëŸ°ì— ëŒ€í•œ ì˜í–¥ì˜ ê°•ë„(ë° ê·¸ ë°©í–¥: í¥ë¶„ì„±(ì–‘ì„± ê°€ì¤‘ì¹˜) ë˜ëŠ” ì–µì œì„±(ìŒì„± ê°€ì¤‘ì¹˜))ë¥¼ ì œì–´í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.
- ê¸°ë³¸ ëª¨ë¸ì—ì„œ ìˆ˜ìƒëŒê¸°ëŠ” ëª¨ë‘ í•©ì‚°ë˜ëŠ” ì„¸í¬ì²´ë¡œ ì‹ í˜¸ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
- ìµœì¢… í•©ê³„ê°€ íŠ¹ì • ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ë‰´ëŸ°ì´ ë°œí™”í•˜ì—¬ ì¶•ì‚­ì„ ë”°ë¼ ìŠ¤íŒŒì´í¬ë¥¼ ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê³„ì‚° ëª¨ë¸ì—ì„œ ìŠ¤íŒŒì´í¬ì˜ ì •í™•í•œ íƒ€ì´ë°ì€ ì¤‘ìš”í•˜ì§€ ì•Šìœ¼ë©° ë°œì‚¬ ë¹ˆë„ë§Œì´ ì •ë³´ë¥¼ ì „ë‹¬í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
- ì´ ì†ë„ ì½”ë“œ í•´ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶•ì‚­ì„ ë”°ë¼ ìŠ¤íŒŒì´í¬ì˜ ë¹ˆë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í™œì„±í™” í•¨ìˆ˜ fë¡œ ë‰´ëŸ°ì˜ ë°œì‚¬ ì†ë„ë¥¼ ëª¨ë¸ë§í•©ë‹ˆë‹¤.
- ì—­ì‚¬ì ìœ¼ë¡œ í™œì„±í™” í•¨ìˆ˜ì˜ ì¼ë°˜ì ì¸ ì„ íƒì€ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ Ïƒì…ë‹ˆë‹¤. ì´ëŠ” ì‹¤ì œ ê°’ ì…ë ¥(í•©ê³„ ì´í›„ì˜ ì‹ í˜¸ ê°•ë„)ì„ ì·¨í•˜ê³  ì´ë¥¼ 0ê³¼ 1 ì‚¬ì´ì˜ ë²”ìœ„ë¡œ ìŠ¤ì¿¼ì‹œí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ í™œì„±í™” í•¨ìˆ˜ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ë‚˜ì¤‘ì— ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì´ êµ¬ì—­.

![kyumly]({{site.url}}/images/cs231/ch6-01.png){: .align-center .img-width-half}


An example code for forward-propagating a single neuron might look as follows:
```python
import numpy as np
import math
class Neuron(object):
    # ... 
    def forward(self, inputs):
        """ assume inputs and weights are 1-D numpy arrays and bias is a number """
        cell_body_sum = np.sum(inputs * self.weights) + self.bias
        firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum))  # sigmoid activation function
        return firing_rate
```
In other words, each neuron performs a dot product with the input and its weights, adds the bias and applies the non-linearity (or activation function), in this case the sigmoid Ïƒ(x)=1/(1+eâˆ’x). 
We will go into more details about different activation functions at the end of this section.

- ì¦‰, ê° ë‰´ëŸ°ì€ ì…ë ¥ê³¼ ê°€ì¤‘ì¹˜ë¡œ ë‚´ì ì„ ìˆ˜í–‰í•˜ê³  ë°”ì´ì–´ìŠ¤ë¥¼ ì¶”ê°€í•˜ê³  ë¹„ì„ í˜•ì„±(ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜)ì„ ì ìš©í•©ë‹ˆë‹¤. ì´ ê²½ìš° ì‹œê·¸ëª¨ì´ë“œ Ïƒ(x)=1/(1+eâˆ’x). 
- ì´ ì„¹ì…˜ì˜ ëì—ì„œ ë‹¤ì–‘í•œ í™œì„±í™” ê¸°ëŠ¥ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤.


Coarse model. <br>
Itâ€™s important to stress that this model of a biological neuron is very coarse:<br>
For example, there are many different types of neurons, each with different properties. The dendrites in biological neurons perform complex nonlinear computations.<br> 
The synapses are not just a single weight, theyâ€™re a complex non-linear dynamical system.<br>
The exact timing of the output spikes in many systems is known to be important, suggesting that the rate code approximation may not hold.<br> 
Due to all these and many other simplifications, be prepared to hear groaning sounds from anyone with some neuroscience background if you draw analogies between Neural Networks and real brains.<br>
See this review (pdf), or more recently this review if you are interested.<br>

- ê±°ì¹œ ëª¨ë¸. ì´ ìƒë¬¼í•™ì  ë‰´ëŸ° ëª¨ë¸ì´ ë§¤ìš° ê±°ì¹ ë‹¤ëŠ” ì ì„ ê°•ì¡°í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. 
- ì˜ˆë¥¼ ë“¤ì–´, ê°ê° ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§„ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë‰´ëŸ°ì´ ìˆìŠµë‹ˆë‹¤. ìƒë¬¼í•™ì  ë‰´ëŸ°ì˜ ìˆ˜ìƒëŒê¸°ëŠ” ë³µì¡í•œ ë¹„ì„ í˜• ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- ì‹œëƒ…ìŠ¤ëŠ” ë‹¨ìˆœí•œ ê°€ì¤‘ì¹˜ê°€ ì•„ë‹ˆë¼ ë³µì¡í•œ ë¹„ì„ í˜• ë™ì  ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ë§ì€ ì‹œìŠ¤í…œì—ì„œ ì¶œë ¥ ìŠ¤íŒŒì´í¬ì˜ ì •í™•í•œ íƒ€ì´ë°ì€ ì¤‘ìš”í•œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìœ¼ë©°, ì´ëŠ” ë¹„ìœ¨ ì½”ë“œ ê·¼ì‚¬ì¹˜ê°€ ìœ ì§€ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. 
- ì´ëŸ¬í•œ ëª¨ë“  ë‹¨ìˆœí™”ì™€ ë‹¤ë¥¸ ë§ì€ ë‹¨ìˆœí™”ë¡œ ì¸í•´ ì‹ ê²½ë§ê³¼ ì‹¤ì œ ë‘ë‡Œ ì‚¬ì´ì— ìœ ì¶”ë¥¼ ê·¸ë¦¬ëŠ” ê²½ìš° ì‹ ê²½ ê³¼í•™ ë°°ê²½ì„ ê°€ì§„ ì‚¬ëŒì˜ í˜ë“¦ ì†Œë¦¬ë¥¼ ë“¤ì„ ì¤€ë¹„ë¥¼ í•˜ì‹­ì‹œì˜¤. 
- ì´ ë¦¬ë·°(pdf)ë¥¼ ì°¸ì¡°í•˜ê±°ë‚˜ ê´€ì‹¬ì´ ìˆëŠ” ê²½ìš° ìµœê·¼ ì´ ë¦¬ë·°ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

## 2.2 Single neuron as a linear classifier
The mathematical form of the model Neuronâ€™s forward computation might look familiar to you.
As we saw with linear classifiers, a neuron has the capacity to â€œlikeâ€ (activation near one) or â€œdislikeâ€ (activation near zero) certain linear regions of its input space. 
Hence, with an appropriate loss function on the neuronâ€™s output, we can turn a single neuron into a linear classifier:

- ëª¨ë¸ Neuronì˜ ìˆœë°©í–¥ ê³„ì‚°ì˜ ìˆ˜í•™ì  í˜•ì‹ì´ ì¹œìˆ™í•´ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì„ í˜• ë¶„ë¥˜ê¸°ì—ì„œ ë³´ì•˜ë“¯ì´ ë‰´ëŸ°ì€ ì…ë ¥ ê³µê°„ì˜ íŠ¹ì • ì„ í˜• ì˜ì—­ì„ "ì¢‹ì•„"(1ì— ê°€ê¹Œìš´ í™œì„±í™”) ë˜ëŠ” "ì‹«ì–´ìš”"(0ì— ê°€ê¹Œìš´ í™œì„±í™”)í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ ìˆìŠµë‹ˆë‹¤.
- ë”°ë¼ì„œ ë‰´ëŸ°ì˜ ì¶œë ¥ì— ëŒ€í•œ ì ì ˆí•œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ ë‰´ëŸ°ì„ ì„ í˜• ë¶„ë¥˜ê¸°ë¡œ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


### 2.2.1 Binary Softmax classifier. 
For example, we can interpret \\(\sigma(\sum_iw_ix_i + b)\\)to be the probability of one of the classes  \\(P(y_i = 1 \mid x_i; w) \\).<br>
The probability of the other class would be \\(P(y_i = 0 \mid x_i; w) = 1 - P(y_i = 1 \mid x_i; w) \\), since they must sum to one.<br>
With this interpretation, we can formulate the cross-entropy loss as we have seen in the Linear Classification section, and optimizing it would lead to a binary Softmax classifier (also known as logistic regression).<br>
Since the sigmoid function is restricted to be between 0-1, the predictions of this classifier are based on whether the output of the neuron is greater than 0.5.<br>

- ì´ì§„ Softmax ë¶„ë¥˜ê¸°. ì˜ˆë¥¼ ë“¤ì–´, \\(\sigma(\sum_iw_ix_i + b)\\)ë¥¼ í´ë˜ìŠ¤ \\(P(y_i = 1 \mid x_i; w) \\)ì¤‘ í•˜ë‚˜ì˜ í™•ë¥ ë¡œ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë‹¤ë¥¸ í´ë˜ìŠ¤ì˜ í™•ë¥ ì€ í•©ì´ 1ì´ì–´ì•¼ í•˜ë¯€ë¡œ \\(P(y_i = 0 \mid x_i; w) = 1 - P(y_i = 1 \mid x_i; w) \\),ì…ë‹ˆë‹¤. 
- ì´ í•´ì„ì„ í†µí•´ ì„ í˜• ë¶„ë¥˜ ì„¹ì…˜ì—ì„œ ë³¸ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ê³µì‹í™”í•  ìˆ˜ ìˆìœ¼ë©° ì´ë¥¼ ìµœì í™”í•˜ë©´ ì´ì§„ Softmax ë¶„ë¥˜ê¸°(ë¡œì§€ìŠ¤í‹± íšŒê·€ë¼ê³ ë„ í•¨)ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” 0-1 ì‚¬ì´ë¡œ ì œí•œë˜ë¯€ë¡œ ì´ ë¶„ë¥˜ê¸°ì˜ ì˜ˆì¸¡ì€ ë‰´ëŸ°ì˜ ì¶œë ¥ì´ 0.5ë³´ë‹¤ í°ì§€ ì—¬ë¶€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.


### 2.2.2 Binary SVM classifier.
Alternatively, we could attach a max-margin hinge loss to the output of the neuron and train it to become a binary Support Vector Machine.<br>

- ì´ì§„ SVM ë¶„ë¥˜ê¸°. ë˜ëŠ” ë‰´ëŸ°ì˜ ì¶œë ¥ì— ìµœëŒ€ ë§ˆì§„ ê²½ì²© ì†ì‹¤ì„ ì²¨ë¶€í•˜ê³  ì´ì§„ ì§€ì› ë²¡í„° ë¨¸ì‹ ì´ ë˜ë„ë¡ í›ˆë ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 2.2.3 Regularization interpretation.
The regularization loss in both SVM/Softmax cases could in this biological view be interpreted as gradual forgetting, since it would have the effect of driving all synaptic weights w towards zero after every parameter update.<br>

- ì •ê·œí™” í•´ì„. SVM/Softmax ì‚¬ë¡€ ëª¨ë‘ì—ì„œ ì •ê·œí™” ì†ì‹¤ì€ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ í›„ì— ëª¨ë“  ì‹œëƒ…ìŠ¤ ê°€ì¤‘ì¹˜ wë¥¼ 0ìœ¼ë¡œ ìœ ë„í•˜ëŠ” íš¨ê³¼ê°€ ìˆê¸° ë•Œë¬¸ì— ì´ ìƒë¬¼í•™ì  ê´€ì ì—ì„œ ì ì§„ì ì¸ ë§ê°ìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 2.3 Commonly used activation functions
Every activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice:

- ëª¨ë“  í™œì„±í™” í•¨ìˆ˜(ë˜ëŠ” ë¹„ì„ í˜•ì„±)ëŠ” ë‹¨ì¼ ìˆ«ìë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ê³ ì • ìˆ˜í•™ ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ ì ‘í•  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ í™œì„±í™” í•¨ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤.
![kyumly]({{site.url}}/images/cs231/ch6-02.png){: .align-center .img-width-half}


### 2.3.1 Sigmoid.
The sigmoid non-linearity has the mathematical **form \\(\sigma(x) = 1 / (1 + e^{-x})\\)** and is shown in the image above on the left. 
As alluded to in the previous section, it takes a real-valued number and â€œsquashesâ€ it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1. 
The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: 
from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). 
In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks:

- ì‹œê·¸ëª¨ì´ë“œ
- ì‹œê·¸ëª¨ì´ë“œ ë¹„ì„ í˜•ì„± ìˆ˜í•™ì  í˜•ì‹ì€ \\(\sigma(x) = 1 / (1 + e^{-x})\\)ì´ë©° ìœ„ì˜ ì™¼ìª½ ì´ë¯¸ì§€ì— í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. 
- ì´ì „ ì„¹ì…˜ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ì‹¤ìˆ˜ ê°’ì„ ê°€ì ¸ì™€ 0ê³¼ 1 ì‚¬ì´ì˜ ë²”ìœ„ë¡œ "ì••ì¶•"í•©ë‹ˆë‹¤. íŠ¹íˆ í° ìŒìˆ˜ëŠ” 0ì´ ë˜ê³  í° ì–‘ìˆ˜ëŠ” 1ì´ ë©ë‹ˆë‹¤. 
- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ì—­ì‚¬ì ìœ¼ë¡œ ìì£¼ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ê²ƒì€ ë‰´ëŸ°ì˜ ë°œí™” ì†ë„ë¡œ ì˜ í•´ì„ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 
- xê°€ ìŒìˆ˜ë¡œ ê°€ê¹Œì›Œì§ˆ ìˆ˜ ë¡ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ê³ , xê°€ ì–‘ìˆ˜ë¡œ ê°ˆ ìˆ˜ ë¡ 1ë¡œ ìˆ˜ë ´í•˜ê²Œ ëœë‹¤.(ê¸°ìš¸ê¸° ì†ì‹¤ë¡œ ì¼ì–´ë‚  ìˆ˜ ìˆìŒ)
- ì‹¤ì œë¡œ ì‹œê·¸ëª¨ì´ë“œ ë¹„ì„ í˜•ì„±ì€ ìµœê·¼ ì„ í˜¸ë„ê°€ ë–¨ì–´ì¡Œìœ¼ë©° ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‘ ê°€ì§€ ì£¼ìš” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.


Sigmoids saturate and kill gradients.
A very undesirable property of the sigmoid neuron is that when **the neuronâ€™s activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero.**
Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gateâ€™s output for the whole objective.
Therefore, if the local gradient is very small, it will effectively â€œkillâ€ the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. 
Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn.

ìš©ì–´
- í¬í™”ëœ(saturated : ì…ë ¥ ê°’ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì„ ë•Œ ì¶œë ¥ê°’ ê·¸ë˜í”„ì˜ ê¸°ìš¸ê¸°ê°€ 0ì— ê°€ê¹Œì›Œì§€ëŠ” í˜„ìƒ (í™œì„±í™” í•¨ìˆ˜ì—ì„œ))

- ê·¹ë‹¨ì ì¸ ì‹œê·¸ëª¨ì´ë“œëŠ” ê¸°ìš¸ê¸°ê°€ ì£½ëŠ”ë‹¤.
- ì‹œê·¸ëª¨ì´ë“œ ë‰´ëŸ°ì˜ ë§¤ìš° ë°”ëŒì§í•˜ì§€ ì•Šì€ íŠ¹ì„±ì€ ë‰´ëŸ°ì˜ í™œì„±í™”ê°€ê°’ì´ ì…ë ¥ê°’ì´ ë„ˆë¬´ ì‘ê±°ë‚˜ ë§¤ìš° í´ ë•Œ ê¸°ìš¸ê¸°ëŠ” ê±°ì˜ 0ì´ë‹¤.
- ì—­ì „íŒŒ ë™ì•ˆ (ë¡œì»¬) ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ì „ì²´ ëª©í‘œì— ëŒ€í•œ ì´ ê²Œì´íŠ¸ ì¶œë ¥ì˜ ê·¸ë˜ë””ì–¸íŠ¸ì— ê³±í•´ì§‘ë‹ˆë‹¤.(ë°‘ì— ê·¸ëŸ¼ ì˜ˆì‹œ)
- ë”°ë¼ì„œ ë¡œì»¬ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ë§¤ìš° ì‘ìœ¼ë©´ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ "ì£½ì´ê³ " ë‰´ëŸ°ì„ í†µí•´ ê°€ì¤‘ì¹˜ë¡œ, ì¬ê·€ì ìœ¼ë¡œ ë°ì´í„°ë¡œ ì‹ í˜¸ê°€ ê±°ì˜ íë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ë˜í•œ í¬í™”ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‹œê·¸ëª¨ì´ë“œ ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•  ë•Œ ê°ë³„í•œ ì£¼ì˜ë¥¼ ê¸°ìš¸ì—¬ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ˆê¸° ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ í¬ë©´ ëŒ€ë¶€ë¶„ì˜ ë‰´ëŸ°ì´ í¬í™” ìƒíƒœê°€ ë˜ì–´ ë„¤íŠ¸ì›Œí¬ê°€ ê±°ì˜ í•™ìŠµí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

![kyumly]({{site.url}}/images/cs231/ch6-03.png){: .align-center .img-width-half}


ì •ë¦¬
- sigmoid í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ê·¹ë‹¨ì ìœ¼ë¡œ ê°’ì´ ì„¤ì • ë ì‹œ ê¸°ìš¸ê¸°ê°€ ì£½ëŠ” í˜„ìƒì´ ë°œìƒí•œë‹¤.
- ê¸°ìš¸ê¸°ë¥¼ ì‚´ë¦¬ê¸° ìœ„í•´ì„œëŠ” ì ì ˆí•œ ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•´ì•¼ í•œë‹¤.


Sigmoid outputs are not zero-centered.<br>
This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered.<br> 
This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. x>0 elementwise in f=wTx+b)), then the gradient on the weights w will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression f).<br>
This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. <br>
However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue.<br>
Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.<br>

- ì‹œê·¸ëª¨ì´ë“œ ì¶œë ¥ì€ 0 ì¤‘ì‹¬ì´ ì•„ë‹™ë‹ˆë‹¤.
- ì´ê²ƒì€ Neural Network(ê³§ ìì„¸íˆ ì„¤ëª…)ì—ì„œ ì²˜ë¦¬ì˜ í›„ë°˜ ê³„ì¸µì— ìˆëŠ” ë‰´ëŸ°ì´ 0 ì¤‘ì‹¬ì´ ì•„ë‹Œ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•˜ê¸° ë•Œë¬¸ì— ë°”ëŒì§í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ê¸°ìš¸ê¸°ê°€ ê°ì†Œ í•˜ëŠ” ë™ì•ˆ ì—­í•™ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ë‰´ëŸ°ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ê°€ í•­ìƒ ì–‘ìˆ˜ì´ë©´(ì˜ˆ: f=wTx+bì—ì„œ ìš”ì†Œë³„ë¡œ x>0) ì—­ì „íŒŒ ë™ì•ˆ ê°€ì¤‘ì¹˜ wì˜ ê¸°ìš¸ê¸°ëŠ” ëª¨ë‘ ì–‘ìˆ˜ ë˜ëŠ” ëª¨ë‘ ìŒìˆ˜(ì „ì²´ ì‹ fì˜ ê¸°ìš¸ê¸°ì— ë”°ë¼ ë‹¤ë¦„).
- ì´ëŠ” ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ì—ì„œ ë°”ëŒì§í•˜ì§€ ì•Šì€ ì§€ê·¸ì¬ê·¸ ì—­í•™ì„ ë„ì…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ê¸°ìš¸ê¸°ê°€ ë°ì´í„° ë°°ì¹˜ì— í•©ì‚°ë˜ë©´ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ìµœì¢… ì—…ë°ì´íŠ¸ì— ë³€ìˆ˜ ë¶€í˜¸ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ ë¬¸ì œê°€ ì–´ëŠ ì •ë„ ì™„í™”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë”°ë¼ì„œ ì´ëŠ” ë¶ˆí¸í•˜ì§€ë§Œ ìœ„ì˜ í¬í™” í™œì„±í™” ë¬¸ì œì— ë¹„í•´ ëœ ì‹¬ê°í•œ ê²°ê³¼ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤.

ì •ë¦¬
- ë§Œì•½ 0ì¤‘ì‹¬ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— í•­ìƒ ì–‘ìˆ˜ë¥¼ ë°”ë¼ë³´ê³  ìˆë‹¤. 
- í™œì„±í™” í•¨ìˆ˜ì—ì„œ ë‚˜ì˜¨ ê°’ì€ ë‹¤ë¥¸ ë‰´ëŸ°ì— í™œì„±í™” í•¨ìˆ˜ê°€ ë˜ê¸° ë•Œë¬¸ì— ë¬´ì €ê±´ ì–‘ìˆ˜ì¸ ê°’ì„ ê°€ì§€ê²Œ ëœë‹¤.
- ê·¸ëŸ¼( dL / dW == dL / df) ë¶€í˜¸ëŠ” ê°™ì•„ì•¼í•œë‹¤.
- dl / df ë¶€í˜¸ë¥¼ ë”°ë¼ê°€ ì „ë¶€ ì–‘ìˆ˜ì´ê±°ë‚˜, ì „ë¶€ ìŒìˆ˜ê°€ ëœë‹¤.

<div>
    ì‹œê·¸ëª¨ì´ë“œëŠ” ë¬¸ì œì ì€ ì´ 3ê°€ì§€ê°€ ìˆë‹¤. ì‹œê·¸ëª¨ì´ë“œ ê·¹ë‹¨ì ì¸ ê°’ì‚¬ìš©, 0 ì¤‘ì‹¬ì´ ì•„ë‹Œ ê°’, exp ê³„ì‚°í•˜ëŠ” ë¹„ìš©ì´ ë§ì´ ë°œìƒí•œë‹¤.
</div>


### 2.3.2 Tanh
The tanh non-linearity is shown on the image above on the right.<br>
It squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered.<br> 
Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.<br>
Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: tanh(x)=2Ïƒ(2x)âˆ’1.<br>



- tanh ë¹„ì„ í˜•ì„±ì€ ì˜¤ë¥¸ìª½ ìœ„ì˜ ì´ë¯¸ì§€ì— í‘œì‹œë©ë‹ˆë‹¤. 
- ì‹¤ìˆ˜ ê°’ì„ [-1, 1] ë²”ìœ„ë¡œ ìŠ¤ì¿¼ì‹œí•©ë‹ˆë‹¤. 
- ì‹œê·¸ëª¨ì´ë“œ ë‰´ëŸ°ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ í™œì„±í™”ëŠ” í¬í™”ë˜ì§€ë§Œ ì‹œê·¸ëª¨ì´ë“œ ë‰´ëŸ°ê³¼ ë‹¬ë¦¬ ì¶œë ¥ì€ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì‹¤ì œë¡œ tanh ë¹„ì„ í˜•ì„±ì€ í•­ìƒ Sìí˜• ë¹„ì„ í˜•ì„±ë³´ë‹¤ ì„ í˜¸ë©ë‹ˆë‹¤. 
- ë˜í•œ tanh ë‰´ëŸ°ì€ ë‹¨ìˆœíˆ ìŠ¤ì¼€ì¼ë§ëœ ì‹œê·¸ëª¨ì´ë“œ ë‰´ëŸ°ì´ë©° íŠ¹íˆ tanh(x)=2Ïƒ(2x)âˆ’1ì´ ì„±ë¦½í•©ë‹ˆë‹¤.


### 2.3.3 Relu


ReLU. The Rectified Linear Unit has become very popular in the last few years. 
It computes the function f(x)=max(0,x). In other words, the activation is simply thresholded at zero (see image above on the left).
There are several pros and cons to using the ReLUs:

- ReLU. Rectified Linear Unitì€ ì§€ë‚œ ëª‡ ë…„ ë™ì•ˆ ë§¤ìš° ì¸ê¸°ë¥¼ ëŒì—ˆìŠµë‹ˆë‹¤. 
- í•¨ìˆ˜ f(x)=max(0,x)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì¦‰, í™œì„±í™”ëŠ” ë‹¨ìˆœíˆ 0ìœ¼ë¡œ ì„ê³„ê°’ì´ ì§€ì •ë©ë‹ˆë‹¤(ì™¼ìª½ ìœ„ ì´ë¯¸ì§€ ì°¸ì¡°). 
- ReLU ì‚¬ìš©ì—ëŠ” ëª‡ ê°€ì§€ ì¥ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.

![kyumly]({{site.url}}/images/cs231/ch6-04.png){: .align-center .img-width-half}


(+) It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. 
It is argued that this is due to its linear, non-saturating form.
- ì‹œê·¸ëª¨ì´ë“œ/tanh í•¨ìˆ˜ì™€ ë¹„êµí•˜ì—¬ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ì˜ ìˆ˜ë ´ì„ í¬ê²Œ ê°€ì†í™”(ì˜ˆ: Krizhevsky et al.ì—ì„œ 6ë°°)í•˜ëŠ” ê²ƒìœ¼ë¡œ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤.
- ì´ëŠ” ì„ í˜•ì ì´ê³  ë¹„í¬í™”ì ì¸ í˜•íƒœ ë•Œë¬¸ì´ë¼ê³  ì£¼ì¥ë©ë‹ˆë‹¤.


(+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.
- (+) ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì‘ì—…(ì§€ìˆ˜ ë“±)ì„ í¬í•¨í•˜ëŠ” tanh/sigmoid ë‰´ëŸ°ê³¼ ë¹„êµí•  ë•Œ ReLUëŠ” ë‹¨ìˆœíˆ í™œì„±í™” ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ 0ìœ¼ë¡œ ì„ê³„ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

(-) Unfortunately, ReLU units can be fragile during training and can â€œdieâ€.<br>
For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again.<br>
If this happens, then the gradient flowing through the unit will forever be zero from that point on. <br>
That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold.<br>
For example, you may find that as much as 40% of your network can be â€œdeadâ€ (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high.<br> 
With a proper setting of the learning rate this is less frequently an issue.<br>


- (-) ë¶ˆí–‰í•˜ê²Œë„ ReLU ìœ ë‹›ì€ í›ˆë ¨ ì¤‘ì— ê¹¨ì§€ê¸° ì‰½ê³  "ì£½ì„" ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- ì˜ˆë¥¼ ë“¤ì–´, ReLU ë‰´ëŸ°ì„ í†µí•´ íë¥´ëŠ” í° ê¸°ìš¸ê¸°ëŠ” ë‰´ëŸ°ì´ ë‹¤ì‹œëŠ” ì–´ë–¤ ë°ì´í„° í¬ì¸íŠ¸ì—ì„œë„ í™œì„±í™”ë˜ì§€ ì•ŠëŠ” ë°©ì‹ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì´ëŸ° ì¼ì´ ë°œìƒí•˜ë©´ ì¥ì¹˜ë¥¼ í†µí•´ íë¥´ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ëŠ” í•´ë‹¹ ì§€ì ë¶€í„° ì˜ì›íˆ 0ì´ ë©ë‹ˆë‹¤.
- ì¦‰, ReLU ìœ ë‹›ì€ ë°ì´í„° ë§¤ë‹ˆí´ë“œì—ì„œ ë–¨ì–´ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í›ˆë ¨ ì¤‘ì— ëŒì´í‚¬ ìˆ˜ ì—†ê²Œ ì£½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- ì˜ˆë¥¼ ë“¤ì–´ í•™ìŠµ ì†ë„ê°€ ë„ˆë¬´ ë†’ê²Œ ì„¤ì •ë˜ë©´ ë„¤íŠ¸ì›Œí¬ì˜ 40%ê°€ "ì£½ì„" ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì¦‰, ì „ì²´ êµìœ¡ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í™œì„±í™”ë˜ì§€ ì•ŠëŠ” ë‰´ëŸ°). 
- í•™ìŠµ ì†ë„ë¥¼ ì ì ˆí•˜ê²Œ ì„¤ì •í•˜ë©´ ë¬¸ì œê°€ ëœ ìì£¼ ë°œìƒí•©ë‹ˆë‹¤.


### 2.3.4 Leaky ReLU
Leaky ReLU.
Leaky ReLUs are one attempt to fix the â€œdying ReLUâ€ problem. <br>
Instead of the function being zero when x < 0, a leaky ReLU will instead have a small positive slope (of 0.01, or so). <br> 
That is, the function computes f(x)=ğŸ™(x<0)(Î±x)+ğŸ™(x>=0)(x) where Î± is a small constant. <br>
Some people report success with this form of activation function, but the results are not always consistent.<br>
The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in Delving Deep into Rectifiers, by Kaiming He et al., 2015.<br> 
However, the consistency of the benefit across tasks is presently unclear.

- ìƒˆëŠ” ReLU. Leaky ReLUëŠ” "ì£½ì–´ê°€ëŠ” ReLU" ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•˜ë‚˜ì˜ ì‹œë„ì…ë‹ˆë‹¤. 
- x < 0ì¼ ë•Œ í•¨ìˆ˜ê°€ 0ì´ ë˜ëŠ” ëŒ€ì‹  ìƒˆëŠ” ReLUëŠ” ì‘ì€ ì–‘ì˜ ê¸°ìš¸ê¸°(0.01 ì •ë„)ë¥¼ ê°–ìŠµë‹ˆë‹¤. 
- ì¦‰, í•¨ìˆ˜ëŠ” f(x)=ğŸ™(x<0)(Î±x)+ğŸ™(x>=0)(x)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ Î±ëŠ” ì‘ì€ ìƒìˆ˜ì…ë‹ˆë‹¤. 
- ì¼ë¶€ ì‚¬ëŒë“¤ì€ ì´ëŸ¬í•œ í˜•íƒœì˜ í™œì„±í™” ê¸°ëŠ¥ìœ¼ë¡œ ì„±ê³µí–ˆë‹¤ê³  ë³´ê³ í•˜ì§€ë§Œ ê²°ê³¼ê°€ í•­ìƒ ì¼ì¹˜í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.
- ìŒìˆ˜ ì˜ì—­ì˜ ê¸°ìš¸ê¸°ëŠ” Kaiming He et al., 2015ì˜ Delving Deep into Rectifiersì— ë„ì…ëœ PReLU ë‰´ëŸ°ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ê° ë‰´ëŸ°ì˜ ë§¤ê°œë³€ìˆ˜ë¡œ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. 
- ê·¸ëŸ¬ë‚˜ ì‘ì—… ì „ë°˜ì— ê±¸ì¹œ ì´ì ì˜ ì¼ê´€ì„±ì€ í˜„ì¬ ë¶ˆë¶„ëª…í•˜ë‹¤.

### 2.3.5 Maxout
Maxout.
Other types of units have been proposed that do not have the functional form f(wTx+b) where a non-linearity is applied on the dot product between the weights and the data.<br>
One relatively popular choice is the Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version.<br>
The Maxout neuron computes the function max(wT1x+b1,wT2x+b2). <br>
Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have w1,b1=0).<br> 
The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU).<br> 
However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters.<br>


- ê°€ì¤‘ì¹˜ì™€ ë°ì´í„° ì‚¬ì´ì˜ ë‚´ì ì— ë¹„ì„ í˜•ì„±ì´ ì ìš©ë˜ëŠ” í•¨ìˆ˜ í˜•ì‹ f(wTx+b)ê°€ ì—†ëŠ” ë‹¤ë¥¸ ìœ í˜•ì˜ ë‹¨ìœ„ê°€ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.
- ìƒëŒ€ì ìœ¼ë¡œ ì¸ê¸° ìˆëŠ” ì„ íƒ ì¤‘ í•˜ë‚˜ëŠ” ReLUì™€ Leaky ë²„ì „ì„ ì¼ë°˜í™”í•˜ëŠ” Maxout ë‰´ëŸ°(ìµœê·¼ Goodfellow et al.ì— ì˜í•´ ë„ì…ë¨)ì…ë‹ˆë‹¤. Maxout ë‰´ëŸ°ì€ í•¨ìˆ˜ max(wT1x+b1,wT2x+b2)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
- ReLUì™€ Leaky ReLUëŠ” ëª¨ë‘ ì´ í˜•ì‹ì˜ íŠ¹ìˆ˜í•œ ê²½ìš°ì…ë‹ˆë‹¤(ì˜ˆ: ReLUì˜ ê²½ìš° w1,b1=0ì„)
- ë”°ë¼ì„œ Maxout ë‰´ëŸ°ì€ ReLU ì¥ì¹˜ì˜ ëª¨ë“  ì´ì (ì„ í˜• ì‘ë™ ì²´ì œ, í¬í™” ì—†ìŒ)ì„ ëˆ„ë¦¬ê³  ë‹¨ì (ì£½ì–´ê°€ëŠ” ReLU)ì´ ì—†ìŠµë‹ˆë‹¤.
- ê·¸ëŸ¬ë‚˜ ReLU ë‰´ëŸ°ê³¼ ë‹¬ë¦¬ ëª¨ë“  ë‹¨ì¼ ë‰´ëŸ°ì— ëŒ€í•œ ë§¤ê°œë³€ìˆ˜ ìˆ˜ê°€ ë‘ ë°°ê°€ ë˜ì–´ ì „ì²´ ë§¤ê°œë³€ìˆ˜ ìˆ˜ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤.


This concludes our discussion of the most common types of neurons and their activation functions.
As a last comment, it is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.

- ì´ê²ƒìœ¼ë¡œ ê°€ì¥ ì¼ë°˜ì ì¸ ìœ í˜•ì˜ ë‰´ëŸ°ê³¼ í™œì„±í™” ê¸°ëŠ¥ì— ëŒ€í•œ ë…¼ì˜ë¥¼ ë§ˆì¹©ë‹ˆë‹¤. 
- ë§ˆì§€ë§‰ìœ¼ë¡œ, ê·¼ë³¸ì ì¸ ë¬¸ì œê°€ ì—†ë”ë¼ë„ ë™ì¼í•œ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ ë‰´ëŸ°ì„ í˜¼í•©í•˜ê³  ì¼ì¹˜ì‹œí‚¤ëŠ” ê²ƒì€ ë§¤ìš° ë“œë­…ë‹ˆë‹¤.

TLDR: â€œWhat neuron type should I use?â€ Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of â€œdeadâ€ units in a network.<br>
If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. <br>
Try tanh, but expect it to work worse than ReLU/Maxout.<br>

- TLDR: "ì–´ë–¤ ë‰´ëŸ° ìœ í˜•ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆê¹Œ?" ReLU ë¹„ì„ í˜•ì„±ì„ ì‚¬ìš©í•˜ê³ , í•™ìŠµ ì†ë„ì— ì£¼ì˜í•˜ê³  ë„¤íŠ¸ì›Œí¬ì—ì„œ "ì£½ì€" ë‹¨ìœ„ì˜ ë¹„ìœ¨ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- ì´ê²ƒì´ ê±±ì •ëœë‹¤ë©´ Leaky ReLU ë˜ëŠ” Maxoutì„ ì‚¬ìš©í•´ ë³´ì‹­ì‹œì˜¤. ì‹œê·¸ëª¨ì´ë“œë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. 
- tanhë¥¼ ì‹œë„í•˜ì§€ë§Œ ReLU/Maxoutë³´ë‹¤ ë” ë‚˜ì˜ê²Œ ì‘ë™í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒí•©ë‹ˆë‹¤.


# 3. Neural Network architectures
## 3.1 Layer-wise organization
Neural Networks as neurons in graphs. Neural Networks are modeled as collections of neurons that are connected in an acyclic graph. <br>
In other words, the outputs of some neurons can become inputs to other neurons. <br>
Cycles are not allowed since that would imply an infinite loop in the forward pass of a network.<br>
Instead of an amorphous blobs of connected neurons, Neural Network models are often organized into distinct layers of neurons.<br>
For regular neural networks, the most common layer type is the fully-connected layer in which neurons between two adjacent layers are fully pairwise connected, but neurons within a single layer share no connections.<br>
Below are two example Neural Network topologies that use a stack of fully-connected layers:<br>

- ê·¸ë˜í”„ì˜ ë‰´ëŸ°ìœ¼ë¡œì„œì˜ ì‹ ê²½ë§. ì‹ ê²½ë§ì€ ë¹„ìˆœí™˜ ê·¸ë˜í”„ë¡œ ì—°ê²°ëœ ë‰´ëŸ° ëª¨ìŒìœ¼ë¡œ ëª¨ë¸ë§ë©ë‹ˆë‹¤. 
- ì¦‰, ì¼ë¶€ ë‰´ëŸ°ì˜ ì¶œë ¥ì´ ë‹¤ë¥¸ ë‰´ëŸ°ì˜ ì…ë ¥ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- ì‚¬ì´í´ì€ ë„¤íŠ¸ì›Œí¬ì˜ ì •ë°©í–¥ íŒ¨ìŠ¤ì—ì„œ ë¬´í•œ ë£¨í”„ë¥¼ ì˜ë¯¸í•˜ë¯€ë¡œ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
- ì‹ ê²½ë§ ëª¨ë¸ì€ ì—°ê²°ëœ ë‰´ëŸ°ì˜ ë¬´ì •í˜• ë¸”ë¡­ ëŒ€ì‹  ë³„ë„ì˜ ë‰´ëŸ° ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. 
- ì¼ë°˜ ì‹ ê²½ë§ì˜ ê²½ìš° ê°€ì¥ ì¼ë°˜ì ì¸ ê³„ì¸µ ìœ í˜•ì€ ì¸ì ‘í•œ ë‘ ê³„ì¸µ ì‚¬ì´ì˜ ë‰´ëŸ°ì´ ì™„ì „íˆ ìŒìœ¼ë¡œ ì—°ê²°ë˜ì§€ë§Œ ë‹¨ì¼ ê³„ì¸µ ë‚´ì˜ ë‰´ëŸ°ì€ ì—°ê²°ì„ ê³µìœ í•˜ì§€ ì•ŠëŠ” ì™„ì „ ì—°ê²° ê³„ì¸µì…ë‹ˆë‹¤.
- ë‹¤ìŒì€ ì™„ì „íˆ ì—°ê²°ëœ ê³„ì¸µì˜ ìŠ¤íƒì„ ì‚¬ìš©í•˜ëŠ” ì‹ ê²½ë§ í† í´ë¡œì§€ì˜ ë‘ ê°€ì§€ ì˜ˆì…ë‹ˆë‹¤.

![kyumly]({{site.url}}/images/cs231/ch6-06.png){: .align-center .img-width-half}



### 3.1.1 Naming conventions
Naming conventions. Notice that when we say N-layer neural network, **we do not count the input layer**.<br> 
Therefore, a single-layer neural network describes a **network with no hidden layers** (input directly mapped to output).<br> 
In that sense, you can sometimes hear people say that logistic regression or SVMs are simply a special case of single-layer Neural Networks.<br> 
You may also hear these networks interchangeably referred to as â€œArtificial Neural Networksâ€ (ANN) or â€œMulti-Layer Perceptronsâ€ (MLP). <br>
Many people do not like the analogies between Neural Networks and real brains and prefer to refer to neurons as units.<br>- 

- ëª…ëª… ê·œì¹™. N ë ˆì´ì–´ ì‹ ê²½ë§ì„ ë§í•  ë•Œ ì…ë ¥ ë ˆì´ì–´ë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì‹­ì‹œì˜¤. 
- ë”°ë¼ì„œ ë‹¨ì¼ ê³„ì¸µ ì‹ ê²½ë§ì€ Heddin ì‹ ê²½ë§(ì¶œë ¥ì— ì§ì ‘ ë§¤í•‘ëœ ì…ë ¥)ì´ ì—†ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.
- ê·¸ëŸ° ì˜ë¯¸ì—ì„œ ì‚¬ëŒë“¤ì´ ë¡œì§€ìŠ¤í‹± íšŒê·€ ë˜ëŠ” SVMì´ ë‹¨ìˆœíˆ ë‹¨ì¼ ê³„ì¸µ ì‹ ê²½ë§ì˜ íŠ¹ìˆ˜í•œ ê²½ìš°ë¼ê³  ë§í•˜ëŠ” ê²ƒì„ ê°€ë” ë“¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë„¤íŠ¸ì›Œí¬ë¥¼ "ì¸ê³µ ì‹ ê²½ë§"(ANN) ë˜ëŠ” "ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ "(MLP)ì´ë¼ê³ ë„ í•©ë‹ˆë‹¤. 
- ë§ì€ ì‚¬ëŒë“¤ì€ ì‹ ê²½ë§ê³¼ ì‹¤ì œ ë‘ë‡Œ ì‚¬ì´ì˜ ìœ ì¶”ë¥¼ ì¢‹ì•„í•˜ì§€ ì•Šìœ¼ë©° ë‰´ëŸ°ì„ ë‹¨ìœ„ë¡œ ì–¸ê¸‰í•˜ëŠ” ê²ƒì„ ì„ í˜¸í•©ë‹ˆë‹¤.

### 3.1.2 Output layer.
Unlike all layers in a Neural Network, the output layer neurons most **commonly do not have an activation function** (or you can think of them as having a linear identity activation function). <br>
This is because the last output layer is **usually taken to represent the class scores** (e.g. in classification), which are arbitrary real-valued numbers, or some kind of real-valued target (e.g. in regression).<br>

- ì‹ ê²½ë§ì˜ ëª¨ë“  ë ˆì´ì–´ì™€ ë‹¬ë¦¬ ì¶œë ¥ ë ˆì´ì–´ ë‰´ëŸ°ì—ëŠ” ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ í™œì„±í™”ë£Œ í•¨ìˆ˜ê¸°ëŠ¥ì´ ì—†ìŠµë‹ˆë‹¤. (ë˜ëŠ” ì„ í˜• ID í™œì„±í™” ê¸°ëŠ¥ì´ ìˆë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆìŒ).
- ì´ëŠ” ë§ˆì§€ë§‰ ì¶œë ¥ ë ˆì´ì–´ê°€ ì¼ë°˜ì ìœ¼ë¡œ í´ë˜ìŠ¤ ì ìˆ˜(ì˜ˆ: ë¶„ë¥˜ì—ì„œ)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë° ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í´ë˜ìŠ¤ ì ìˆ˜ëŠ” ì„ì˜ì˜ ì‹¤ì œ ê°’ì´ê±°ë‚˜ ì¼ì¢…ì˜ ì‹¤ì œ ê°’ ëŒ€ìƒ(ì˜ˆ: íšŒê·€)ì…ë‹ˆë‹¤.

### 3.1.3 Sizing neural networks

The two metrics that people commonly use to measure the size of neural networks are the number of neurons, or **more commonly the number of parameters**.<br> 
Working with the two example networks in the above picture:<br>

- ì‚¬ëŒë“¤ì´ ì‹ ê²½ë§ì˜ í¬ê¸°ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²„ì€ ë‰´ëŸ°ì˜ ìˆ˜ ë˜ëŠ” **ì¼ë°˜ì ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ì˜ ìˆ˜ ì…ë‹ˆë‹¤.** 
- ìœ„ ê·¸ë¦¼ì—ì„œ ë‘ ê°€ì§€ ì˜ˆì‹œ ë„¤íŠ¸ì›Œí¬ë¡œ ì‘ì—…:

![kyumly]({{site.url}}/images/cs231/ch6-07.png){: .align-center .img-width-half}



To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning).<br>
However, as we will see the number of effective connections is significantly greater due to parameter sharing. More on this in the Convolutional Neural Networks module.<br>

- ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ìµœì‹  Convolutional Networksì—ëŠ” 1ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©° ì¼ë°˜ì ìœ¼ë¡œ ì•½ 10-20ê°œì˜ ë ˆì´ì–´ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤(ë”°ë¼ì„œ ë”¥ ëŸ¬ë‹).
- ê·¸ëŸ¬ë‚˜ ë§¤ê°œë³€ìˆ˜ ê³µìœ ë¡œ ì¸í•´ íš¨ê³¼ì ì¸ ì—°ê²° ìˆ˜ê°€ í›¨ì”¬ ë” ë§ì•„ì§€ëŠ” ê²ƒì„ ë³´ê²Œ ë  ê²ƒì…ë‹ˆë‹¤. Convolutional Neural Networks ëª¨ë“ˆì—ì„œ ì´ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ì„¸ìš”.

## 3.2 Example feed-forward computation
Repeated matrix multiplications interwoven with activation function.<br>
One of the primary reasons that Neural Networks are organized into layers is that this structure makes it very simple and efficient to evaluate Neural Networks using matrix vector operations.<br> 
Working with the example three-layer neural network in the diagram above, the input would be a [3x1] vector.<br>
All connection strengths for a layer can be stored in a single matrix. For example, the first hidden layerâ€™s weights W1 would be of size [4x3], and the biases for all units would be in the vector b1, of size [4x1].<br> 
Here, every single neuron has its weights in a row of W1, so the matrix vector multiplication np.dot(W1,x) evaluates the activations of all neurons in that layer.<br> 
Similarly, W2 would be a [4x4] matrix that stores the connections of the second hidden layer, and W3 a [1x4] matrix for the last (output) layer. <br>
The full forward pass of this 3-layer neural network is then simply three matrix multiplications, interwoven with the application of the activation function:<br>

- í™œì„±í™” ê¸°ëŠ¥ê³¼ ì§œì—¬ì§„ ë°˜ë³µ í–‰ë ¬ ê³±ì…ˆ. 
- ì‹ ê²½ë§ì´ ê³„ì¸µìœ¼ë¡œ êµ¬ì„±ë˜ëŠ” ì£¼ëœ ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ” ì´ êµ¬ì¡°ê°€ í–‰ë ¬ ë²¡í„° ì—°ì‚°ì„ ì‚¬ìš©í•˜ì—¬ ì‹ ê²½ë§ì„ í‰ê°€í•˜ëŠ” ê²ƒì„ ë§¤ìš° ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
- ìœ„ì˜ ë‹¤ì´ì–´ê·¸ë¨ì—ì„œ ì˜ˆì œ 3ê³„ì¸µ ì‹ ê²½ë§ìœ¼ë¡œ ì‘ì—…í•˜ë©´ ì…ë ¥ì€ [3x1] ë²¡í„°ê°€ ë©ë‹ˆë‹¤. ë ˆì´ì–´ì˜ ëª¨ë“  ì—°ê²° ê°•ë„ëŠ” ë‹¨ì¼ ë§¤íŠ¸ë¦­ìŠ¤ì— ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- ì˜ˆë¥¼ ë“¤ì–´ ì²« ë²ˆì§¸ íˆë“  ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ W1ì€ í¬ê¸°ê°€ [4x3]ì´ê³  ëª¨ë“  ë‹¨ìœ„ì— ëŒ€í•œ ë°”ì´ì–´ìŠ¤ëŠ” í¬ê¸°ê°€ [4x1]ì¸ ë²¡í„° b1ì— ìˆìŠµë‹ˆë‹¤. 
- ì—¬ê¸°ì—ì„œ ëª¨ë“  ë‹¨ì¼ ë‰´ëŸ°ì€ W1ì˜ í–‰ì— ê°€ì¤‘ì¹˜ê°€ ìˆìœ¼ë¯€ë¡œ í–‰ë ¬ ë²¡í„° ê³±ì…ˆ np.dot(W1,x)ëŠ” í•´ë‹¹ ê³„ì¸µì— ìˆëŠ” ëª¨ë“  ë‰´ëŸ°ì˜ í™œì„±í™”ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
- ë§ˆì°¬ê°€ì§€ë¡œ W2ëŠ” ë‘ ë²ˆì§¸ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ì˜ ì—°ê²°ì„ ì €ì¥í•˜ëŠ” [4x4] í–‰ë ¬ì´ê³  W3ì€ ë§ˆì§€ë§‰(ì¶œë ¥) ë ˆì´ì–´ì— ëŒ€í•œ [1x4] í–‰ë ¬ì…ë‹ˆë‹¤. ì´ 3ê³„ì¸µ ì‹ ê²½ë§ì˜ ì „ì²´ ì •ë°©í–¥ í†µê³¼ëŠ” í™œì„±í™” í•¨ìˆ˜ì˜ ì ìš©ê³¼ ì§œì—¬ì§„ 3ê°œì˜ í–‰ë ¬ ê³±ì…ˆì…ë‹ˆë‹¤.

![kyumly]({{site.url}}/images/cs231/ch6-08.png){: .align-center .img-width-half}

In the above code, [W1,W2,W3,b1,b2,b3] are the learnable parameters of the network. <br>
Notice also that instead of having a single input column vector, the variable x could hold an entire batch of training data (where each input example would be a column of x) and then all examples would be efficiently evaluated in parallel.<br>
Notice that the final Neural Network layer usually doesnâ€™t have an activation function (e.g. it represents a (real-valued) class score in a classification setting).<br>

- ìœ„ì˜ ì½”ë“œì—ì„œ [W1,W2,W3,b1,b2,b3]ì€ ë„¤íŠ¸ì›Œí¬ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤. 
- ë˜í•œ ë‹¨ì¼ ì…ë ¥ ì—´ ë²¡í„°ë¥¼ ê°–ëŠ” ëŒ€ì‹  ë³€ìˆ˜ xëŠ” í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë°°ì¹˜(ê° ì…ë ¥ ì˜ˆëŠ” xì˜ ì—´ì´ ë¨)ë¥¼ ë³´ìœ í•  ìˆ˜ ìˆìœ¼ë©° ëª¨ë“  ì˜ˆëŠ” ë³‘ë ¬ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤. 
- ìµœì¢… ì‹ ê²½ë§ ê³„ì¸µì—ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤(ì˜ˆ: ë¶„ë¥˜ ì„¤ì •ì—ì„œ (ì‹¤ì œ ê°’) í´ë˜ìŠ¤ ì ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ„).

## 3.3. Representational power
One way to look at Neural Networks with fully-connected layers is that they define a family of functions that are parameterized by the weights of the network.<br>
A natural question that arises is: What is the representational power of this family of functions?<br> 
In particular, are there functions that cannot be modeled with a Neural Network?<br>


- ì™„ì „íˆ ì—°ê²°ëœ ê³„ì¸µì´ ìˆëŠ” ì‹ ê²½ë§ì„ ë³´ëŠ” í•œ ê°€ì§€ ë°©ë²•ì€ ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ë¡œ ë§¤ê°œ ë³€ìˆ˜í™”ë˜ëŠ” ê¸°ëŠ¥êµ°ì„ ì •ì˜í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.
- ë°œìƒí•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ ê¸°ëŠ¥ ê³„ì—´ì˜ í‘œí˜„ë ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
- íŠ¹íˆ ì‹ ê²½ë§ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ì—†ëŠ” ê¸°ëŠ¥ì´ ìˆìŠµë‹ˆê¹Œ?


It turns out that Neural Networks with at least one hidden layer are universal approximators.<br>
That is, it can be shown (e.g. see Approximation by Superpositions of Sigmoidal Function from 1989 (pdf), or this intuitive explanation from Michael Nielsen) that given any continuous function f(x) and some Ïµ>0, there exists a Neural Network g(x) with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that âˆ€x,âˆ£f(x)âˆ’g(x)âˆ£<Ïµ.<br> 
In other words, the neural network can approximate any continuous function.<br>

 
- ì ì–´ë„ í•˜ë‚˜ì˜ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ê°€ ìˆëŠ” ì‹ ê²½ë§ì€ ë³´í¸ì ì¸ ê·¼ì‚¬ì¹˜ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
- ì¦‰, ì„ì˜ì˜ ì—°ì† í•¨ìˆ˜ f(x)ì™€ ì¼ë¶€ Ïµ>0ì´ ì£¼ì–´ì§€ë©´ (ì˜ˆë¥¼ ë“¤ì–´, ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ì¤‘ì²©ì— ì˜í•œ ê·¼ì‚¬ì¹˜ ë˜ëŠ” ë§ˆì´í´ ë‹ìŠ¨ì˜ ì§ê´€ì  ì„¤ëª… ì°¸ì¡°), âˆ€x, ,xì™€ ê°™ì€ í•˜ë‚˜ì˜ ìˆ¨ê²¨ì§„ ì¸µ(ë¹„ì„ í˜•ì„±ì˜ í•©ë¦¬ì ì¸ ì„ íƒì„ ê°€ì§„)ì„ ê°€ì§„ ì‹ ê²½ë§ g(x)ê°€ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤,ã€Œf(x)-g(x)ã€<.
- ë‹¤ì‹œ ë§í•´, ì‹ ê²½ë§ì€ ëª¨ë“  ì—°ì† í•¨ìˆ˜ì— ê·¼ì ‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


If one hidden layer suffices to approximate any function, why use more layers and go deeper? <br>
The answer is that the fact that a two-layer Neural Network is a universal approximator is, while mathematically cute, a relatively weak and useless statement in practice.<br> 
In one dimension, the â€œsum of indicator bumpsâ€ function g(x)=âˆ‘iciğŸ™(ai<x<bi) where a,b,c are parameter vectors is also a universal approximator,<br>
but noone would suggest that we use this functional form in Machine Learning. <br>
Neural Networks work well in practice because they compactly express nice, smooth functions that fit well with the statistical properties of data we encounter in practice, and are also easy to learn using our optimization algorithms (e.g. gradient descent).<br> 
Similarly, the fact that deeper networks (with multiple hidden layers) can work better than a single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal.<br>

- ìˆ¨ê²¨ì§„ ê³„ì¸µ í•˜ë‚˜ë¡œ ê¸°ëŠ¥ì„ ê·¼ì‚¬í™”í•  ìˆ˜ ìˆë‹¤ë©´ ë” ë§ì€ ê³„ì¸µì„ ì‚¬ìš©í•˜ê³  ë” ê¹Šì´ íŒŒê³ ë“œëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ? 
- ë‹µì€ 2ì¸µ ì‹ ê²½ë§ì´ ë³´í¸ì ì¸ ê·¼ì‚¬ê¸°ë¼ëŠ” ì‚¬ì‹¤ì€ ìˆ˜í•™ì ìœ¼ë¡œ ê·€ì—½ì§€ë§Œ ì‹¤ì œë¡œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì•½í•˜ê³  ì“¸ëª¨ì—†ëŠ” ì§„ìˆ ì´ë¼ëŠ” ê²ƒì…ë‹ˆë‹¤.
- í•˜ë‚˜ì˜ ì°¨ì›ì—ì„œ, a,b,c ë§¤ê°œë³€ìˆ˜ ë²¡í„°ê°€ ë³´í¸ì ì¸ ê·¼ì‚¬ê¸°ì´ê¸°ë„ í•˜ì§€ë§Œ,
- ì•„ë¬´ë„ ê¸°ê³„ í•™ìŠµì—ì„œ ì´ ê¸°ëŠ¥ í˜•íƒœë¥¼ ì‚¬ìš©í•  ê²ƒì„ ì œì•ˆí•˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.
- ì‹ ê²½ë§ì€ ì‹¤ì œì—ì„œ ì˜ ì‘ë™í•˜ëŠ”ë°, ì´ëŠ” ì‹¤ì œì—ì„œ ìš°ë¦¬ê°€ ì ‘í•˜ëŠ” ë°ì´í„°ì˜ í†µê³„ì  íŠ¹ì„±ì— ì˜ ë§ëŠ” ë©‹ì§€ê³  ë§¤ë„ëŸ¬ìš´ í•¨ìˆ˜ë¥¼ ì••ì¶•ì ìœ¼ë¡œ í‘œí˜„í•˜ê³  ìµœì í™” ì•Œê³ ë¦¬ë“¬(ì˜ˆ: ê²½ì‚¬ í•˜ê°•ë²•)ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 
- ë§ˆì°¬ê°€ì§€ë¡œ, (ì—¬ëŸ¬ ê°œì˜ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ê°€ ìˆëŠ”) ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ê°€ ë‹¨ì¼ ì€ë‹‰ ë ˆì´ì–´ ë„¤íŠ¸ì›Œí¬ë³´ë‹¤ ë” ì˜ ì‘ë™í•  ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì€ í‘œí˜„ë ¥ì´ ë™ì¼í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì—ë„ ë¶ˆêµ¬í•˜ê³  ê²½í—˜ì  ê´€ì°°ì…ë‹ˆë‹¤.

 
As an aside, in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more.<br>
This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). <br>
One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain.<br>
 
- ì—¬ë‹´ì´ì§€ë§Œ, ì‹¤ì œë¡œëŠ” 3ê³„ì¸µ ì‹ ê²½ë§ì´ 2ê³„ì¸µ ì‹ ê²½ë§ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì€ ê²½ìš°ê°€ ë§ì§€ë§Œ ë” ê¹Šì€(4,5,6ê³„ì¸µ) ì‹ ê²½ë§ì€ í›¨ì”¬ ë” í° ë„ì›€ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ì´ëŠ” ìš°ìˆ˜í•œ ì¸ì‹ ì‹œìŠ¤í…œ(ì˜ˆ: ì•½ 10ê°œì˜ í•™ìŠµ ê°€ëŠ¥í•œ ê³„ì¸µ)ì— ëŒ€í•´ ê¹Šì´ê°€ ë§¤ìš° ì¤‘ìš”í•œ êµ¬ì„± ìš”ì†Œì¸ ê²ƒìœ¼ë¡œ ë°í˜€ì§„ Convolutional Networksì™€ ê·¹ëª…í•œ ëŒ€ì¡°ë¥¼ ì´ë£¹ë‹ˆë‹¤. 
- ì´ ê´€ì°°ì— ëŒ€í•œ í•œ ê°€ì§€ ì£¼ì¥ì€ ì´ë¯¸ì§€ê°€ ê³„ì¸µ êµ¬ì¡°(ì˜ˆ: ì–¼êµ´ì€ ëˆˆìœ¼ë¡œ êµ¬ì„±ë˜ê³  ê°€ì¥ìë¦¬ë¡œ êµ¬ì„±ë¨ ë“±)ë¥¼ í¬í•¨í•˜ë¯€ë¡œ ì´ ë°ì´í„° ë„ë©”ì¸ì— ëŒ€í•´ ì—¬ëŸ¬ ê³„ì¸µì˜ ì²˜ë¦¬ê°€ ì§ê´€ì ìœ¼ë¡œ ì´í•´ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.


The full story is, of course, much more involved and a topic of much recent research. If you are interested in these topics we recommend for further reading:<br>

- Deep Learning book in press by Bengio, Goodfellow, Courville, in particular Chapter 6.4.<br>
- Do Deep Nets Really Need to be Deep?<br>
- FitNets: Hints for Thin Deep Nets<br>

## 3.4 Setting number of layers and their sizes
How do we decide on what architecture to use when faced with a practical problem?<br> 
Should we use no hidden layers? One hidden layer? Two hidden layers?<br>
How large should each layer be? <br>
First, note that as we increase the size and number of layers in a Neural Network, the capacity of the network increases.<br> 
That is, the space of representable functions grows since the neurons can collaborate to express many different functions. <br>
For example, suppose we had a binary classification problem in two dimensions. <br>
We could train three separate neural networks, each with one hidden layer of some size and obtain the following classifiers:<br>

- ì‹¤ì œ ë¬¸ì œì— ì§ë©´í–ˆì„ ë•Œ ì‚¬ìš©í•  ì•„í‚¤í…ì²˜ë¥¼ ì–´ë–»ê²Œ ê²°ì •í•©ë‹ˆê¹Œ? 
- ìˆ¨ê²¨ì§„ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆê¹Œ? í•˜ë‚˜ì˜ ìˆ¨ê²¨ì§„ ë ˆì´ì–´? 
- ë‘ ê°œì˜ íˆë“  ë ˆì´ì–´? ê° ë ˆì´ì–´ëŠ” ì–¼ë§ˆë‚˜ ì»¤ì•¼ í•©ë‹ˆê¹Œ?
- ì²«ì§¸, ì‹ ê²½ë§ì—ì„œ ë ˆì´ì–´ì˜ í¬ê¸°ì™€ ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ë„¤íŠ¸ì›Œí¬ ìš©ëŸ‰ì´ ì¦ê°€í•©ë‹ˆë‹¤. 
- ì¦‰, ë‰´ëŸ°ì´ í˜‘ë ¥í•˜ì—¬ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í‘œí˜„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í‘œí˜„ ê°€ëŠ¥í•œ ê¸°ëŠ¥ì˜ ê³µê°„ì´ ì»¤ì§‘ë‹ˆë‹¤. 
- ì˜ˆë¥¼ ë“¤ì–´ 2ì°¨ì›ì—ì„œ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œê°€ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. 
- ìš°ë¦¬ëŠ” ê°ê° ì¼ì •í•œ í¬ê¸°ì˜ í•˜ë‚˜ì˜ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ê°€ ìˆëŠ” ì„¸ ê°œì˜ ê°œë³„ ì‹ ê²½ë§ì„ í›ˆë ¨í•˜ê³  ë‹¤ìŒ ë¶„ë¥˜ê¸°ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


![kyumly]({{site.url}}/images/cs231/ch6-09.png){: .align-center .img-width-half}


In the diagram above, we can see that Neural Networks with more neurons can express more complicated functions. <br>
However, this is both a blessing (since we can learn to classify more complicated data) and a curse (since it is easier to overfit the training data).<br> 
Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship.<br>
For example, the model with 20 hidden neurons fits all the training data but at the cost of segmenting the space into many disjoint red and green decision regions.<br> 
The model with 3 hidden neurons only has the representational power to classify the data in broad strokes. <br>
It models the data as two blobs and interprets the few red points inside the green cluster as outliers (noise). In practice, this could lead to better generalization on the test set.<br>


- ìœ„ì˜ ë‹¤ì´ì–´ê·¸ë¨ì—ì„œ ìš°ë¦¬ëŠ” ë” ë§ì€ ë‰´ëŸ°ì„ ê°€ì§„ ì‹ ê²½ë§ì´ ë” ë³µì¡í•œ ê¸°ëŠ¥ì„ í‘œí˜„í•  ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- ê·¸ëŸ¬ë‚˜ ì´ê²ƒì€ ì¶•ë³µ(ë” ë³µì¡í•œ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë°©ë²•ì„ ë°°ìš¸ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—)ì¸ ë™ì‹œì— ì €ì£¼(í›ˆë ¨ ë°ì´í„°ë¥¼ ê³¼ëŒ€ì í•©í•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì—)ì…ë‹ˆë‹¤.
- ìš©ëŸ‰ì´ í° ëª¨ë¸ì´ (ê°€ì •ëœ) ê¸°ë³¸ ê´€ê³„ ëŒ€ì‹  ë°ì´í„°ì˜ ë…¸ì´ì¦ˆë¥¼ ë§ì¶œ ë•Œ ê³¼ì í•©ì´ ë°œìƒí•©ë‹ˆë‹¤. 
- ì˜ˆë¥¼ ë“¤ì–´, 20ê°œì˜ ì€ë‹‰ ë‰´ëŸ°ì´ ìˆëŠ” ëª¨ë¸ì€ ëª¨ë“  êµìœ¡ ë°ì´í„°ì— ì í•©í•˜ì§€ë§Œ ê³µê°„ì„ ë§ì€ ë¶„ë¦¬ëœ ë¹¨ê°„ìƒ‰ ë° ë…¹ìƒ‰ ê²°ì • ì˜ì—­ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ë¹„ìš©ì´ ë“­ë‹ˆë‹¤. 
- ì€ë‹‰ ë‰´ëŸ°ì´ 3ê°œì¸ ëª¨ë¸ì€ ë°ì´í„°ë¥¼ ê´‘ë²”ìœ„í•˜ê²Œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” í‘œí˜„ë ¥ë§Œ ìˆìŠµë‹ˆë‹¤. 
- ë°ì´í„°ë¥¼ ë‘ ê°œì˜ ì–¼ë£©ìœ¼ë¡œ ëª¨ë¸ë§í•˜ê³  ë…¹ìƒ‰ í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì˜ ëª‡ ê°€ì§€ ë¹¨ê°„ìƒ‰ ì ì„ ì´ìƒì¹˜(ë…¸ì´ì¦ˆ)ë¡œ í•´ì„í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ë” ë‚˜ì€ ì¼ë°˜í™”ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Based on our discussion above, it seems that smaller neural networks can be preferred if the data is not complex enough to prevent overfitting.<br>
However, this is incorrect - there are many other preferred ways to prevent overfitting in Neural Networks that we will discuss later (such as L2 regularization, dropout, input noise).<br> 
In practice, it is always better to use these methods to control overfitting instead of the number of neurons.<br>

- ìœ„ì˜ ë…¼ì˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ê°€ ê³¼ì í•©ì„ ë°©ì§€í•  ë§Œí¼ ì¶©ë¶„íˆ ë³µì¡í•˜ì§€ ì•Šì€ ê²½ìš° ë” ì‘ì€ ì‹ ê²½ë§ì´ ì„ í˜¸ë  ìˆ˜ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.
- ê·¸ëŸ¬ë‚˜ ì´ê²ƒì€ ì˜ëª»ëœ ê²ƒì…ë‹ˆë‹¤. ë‚˜ì¤‘ì— ë…¼ì˜í•  ì‹ ê²½ë§ì—ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì„ í˜¸ë˜ëŠ” ë‹¤ë¥¸ ë°©ë²•(ì˜ˆ: L2 ì •ê·œí™”, ë“œë¡­ì•„ì›ƒ, ì…ë ¥ ë…¸ì´ì¦ˆ)ì´ ë§ì´ ìˆìŠµë‹ˆë‹¤.
- ì‹¤ì œë¡œëŠ” ì´ëŸ¬í•œ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë‰´ëŸ° ìˆ˜ ëŒ€ì‹  ê³¼ì í•©ì„ ì œì–´í•˜ëŠ” ê²ƒì´ í•­ìƒ ë” ì¢‹ìŠµë‹ˆë‹¤.



The subtle reason behind this is that smaller networks are harder to train with local methods such as Gradient Descent: <br>
Itâ€™s clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss).<br> 
Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. <br>
Since Neural Networks are non-convex, it is hard to study these properties mathematically, but some attempts to understand these objective functions have been made, e.g. in a recent paper The Loss Surfaces of Multilayer Networks.<br>
In practice, what you find is that if you train a small network the final loss can display a good amount of variance - in some cases you get lucky and converge to a good place but in some cases you get trapped in one of the bad minima.<br>
On the other hand, if you train a large network youâ€™ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization.<br>

- ë¯¸ë¬˜í•œ ì´ìœ ëŠ” ì†Œê·œëª¨ ë„¤íŠ¸ì›Œí¬ëŠ” ê²½ì‚¬ í•˜ê°•ë²•ê³¼ ê°™ì€ ë¡œì»¬ ë°©ë²•ìœ¼ë¡œ í›ˆë ¨í•˜ê¸°ê°€ ë” ì–´ë µë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. 
- ì†ì‹¤ í•¨ìˆ˜ì˜ ë¡œì»¬ ìµœì†Œê°’ì´ ìƒëŒ€ì ìœ¼ë¡œ ì ë‹¤ëŠ” ê²ƒì€ ë¶„ëª…í•˜ì§€ë§Œ ì´ëŸ¬í•œ ìµœì†Œê°’ ì¤‘ ë§ì€ ë¶€ë¶„ì´ ìˆ˜ë ´í•˜ê¸° ë” ì‰½ê³  ê·¸ê²ƒë“¤ì€ ë‚˜ì˜ë‹¤(ì¦‰, ë†’ì€ ì†ì‹¤). 
- ë°˜ëŒ€ë¡œ ë” í° ì‹ ê²½ë§ì€ í›¨ì”¬ ë” ë§ì€ ë¡œì»¬ ìµœì†Œê°’ì„ í¬í•¨í•˜ì§€ë§Œ ì´ëŸ¬í•œ ìµœì†Œê°’ì€ ì‹¤ì œ ì†ì‹¤ ì¸¡ë©´ì—ì„œ í›¨ì”¬ ë” ë‚˜ì€ ê²ƒìœ¼ë¡œ íŒëª…ë˜ì—ˆìŠµë‹ˆë‹¤. 
- ì‹ ê²½ë§ì€ ë³¼ë¡í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ì†ì„±ì„ ìˆ˜í•™ì ìœ¼ë¡œ ì—°êµ¬í•˜ê¸°ëŠ” ì–´ë µì§€ë§Œ ì´ëŸ¬í•œ ëª©ì  í•¨ìˆ˜ë¥¼ ì´í•´í•˜ë ¤ëŠ” ëª‡ ê°€ì§€ ì‹œë„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. 
- ìµœê·¼ ë…¼ë¬¸ The Loss Surfaces of Multilayer Networksì—ì„œ. ì‹¤ì œë¡œ, ë‹¹ì‹ ì´ ë°œê²¬í•œ ê²ƒì€ ì‘ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ê²½ìš° ìµœì¢… ì†ì‹¤ì´ ìƒë‹¹í•œ ì–‘ì˜ ë¶„ì‚°ì„ í‘œì‹œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. 
- ì–´ë–¤ ê²½ìš°ì—ëŠ” ìš´ì´ ì¢‹ì•„ ì¢‹ì€ ê³³ìœ¼ë¡œ ìˆ˜ë ´í•˜ì§€ë§Œ ì–´ë–¤ ê²½ìš°ì—ëŠ” ë‚˜ìœ ìµœì†Œê°’ ì¤‘ í•˜ë‚˜ì— ê°‡íˆê²Œ ë©ë‹ˆë‹¤. 
- ë°˜ë©´ì— ëŒ€ê·œëª¨ ë„¤íŠ¸ì›Œí¬ë¥¼ êµìœ¡í•˜ëŠ” ê²½ìš° ë‹¤ì–‘í•œ ì†”ë£¨ì…˜ì„ ì°¾ê¸° ì‹œì‘í•˜ì§€ë§Œ ìµœì¢… ë‹¬ì„± ì†ì‹¤ì˜ ë¶„ì‚°ì€ í›¨ì”¬ ì‘ì•„ì§‘ë‹ˆë‹¤. ì¦‰, ëª¨ë“  ì†”ë£¨ì…˜ì€ ê±°ì˜ ë™ë“±í•˜ê²Œ ìš°ìˆ˜í•˜ë©° ë¬´ì‘ìœ„ ì´ˆê¸°í™”ì˜ ìš´ì— ëœ ì˜ì¡´í•©ë‹ˆë‹¤.

To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network. We can look at the results achieved by three different settings:
- ë‹¤ì‹œ ë§í•˜ë©´ ì •ê·œí™” ê°•ë„ëŠ” ì‹ ê²½ë§ì˜ ê³¼ì í•©ì„ ì œì–´í•˜ëŠ” ë° ì„ í˜¸ë˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì„¸ ê°€ì§€ ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ ì–»ì€ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.



# 4. Summary

We introduced a very coarse model of a biological neuron.<br>
We discussed several types of activation functions that are used in practice, with ReLU being the most common choice.<br>
We introduced Neural Networks where neurons are connected with Fully-Connected layers where neurons in adjacent layers have full pair-wise connections, but neurons within a layer are not connected.<br>
We saw that this layered architecture enables very efficient evaluation of Neural Networks based on matrix multiplications interwoven with the application of the activation function.<br>
We saw that that Neural Networks are universal function approximators, but we also discussed the fact that this property has little to do with their ubiquitous use. They are used because they make certain â€œrightâ€ assumptions about the functional forms of functions that come up in practice.<br>
We discussed the fact that larger networks will always work better than smaller networks, but their higher model capacity must be appropriately addressed with stronger regularization (such as higher weight decay), or they might overfit. We will see more forms of regularization (especially dropout) in later sections.<br>

- ìƒë¬¼í•™ì  ë‰´ëŸ°ì˜ ë§¤ìš° ê±°ì¹œ ëª¨ë¸ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.
- ìš°ë¦¬ëŠ” ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” ì—¬ëŸ¬ ìœ í˜•ì˜ í™œì„±í™” í•¨ìˆ˜ì— ëŒ€í•´ ë…¼ì˜í–ˆìœ¼ë©° ReLUê°€ ê°€ì¥ ì¼ë°˜ì ì¸ ì„ íƒì…ë‹ˆë‹¤.
- ìš°ë¦¬ëŠ” ì¸ì ‘í•œ ë ˆì´ì–´ì˜ ë‰´ëŸ°ì´ ì „ì²´ ìŒìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆì§€ë§Œ ë ˆì´ì–´ ë‚´ì˜ ë‰´ëŸ°ì€ ì—°ê²°ë˜ì§€ ì•Šì€ ì™„ì „ ì—°ê²° ë ˆì´ì–´ì™€ ë‰´ëŸ°ì´ ì—°ê²°ë˜ëŠ” ì‹ ê²½ë§ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.
- ìš°ë¦¬ëŠ” ì´ ê³„ì¸µí™”ëœ ì•„í‚¤í…ì²˜ê°€ í™œì„±í™” í•¨ìˆ˜ì˜ ì ìš©ê³¼ ì§œì—¬ì§„ í–‰ë ¬ ê³±ì…ˆì„ ê¸°ë°˜ìœ¼ë¡œ ì‹ ê²½ë§ì„ ë§¤ìš° íš¨ìœ¨ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
- ìš°ë¦¬ëŠ” ì‹ ê²½ë§ì´ ë²”ìš© í•¨ìˆ˜ ê·¼ì‚¬ìë¼ëŠ” ê²ƒì„ ë³´ì•˜ì§€ë§Œ ì´ ì†ì„±ì´ ìœ ë¹„ì¿¼í„°ìŠ¤ ì‚¬ìš©ê³¼ ê±°ì˜ ê´€ë ¨ì´ ì—†ë‹¤ëŠ” ì‚¬ì‹¤ì— ëŒ€í•´ì„œë„ ë…¼ì˜í–ˆìŠµë‹ˆë‹¤. ê·¸ê²ƒë“¤ì€ ì‹¤ì œë¡œ ë‚˜íƒ€ë‚˜ëŠ” í•¨ìˆ˜ì˜ ê¸°ëŠ¥ì  í˜•íƒœì— ëŒ€í•´ íŠ¹ì •í•œ "ì˜¬ë°”ë¥¸" ê°€ì •ì„ í•˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
- ìš°ë¦¬ëŠ” ë” í° ë„¤íŠ¸ì›Œí¬ê°€ ë” ì‘ì€ ë„¤íŠ¸ì›Œí¬ë³´ë‹¤ í•­ìƒ ë” ì˜ ì‘ë™í•œë‹¤ëŠ” ì‚¬ì‹¤ì— ëŒ€í•´ ë…¼ì˜í–ˆì§€ë§Œ ë” í° ëª¨ë¸ ìš©ëŸ‰ì€ ë” ê°•ë ¥í•œ ì •ê·œí™”(ì˜ˆ: ë” ë†’ì€ ê°€ì¤‘ì¹˜ ê°ì‡ )ë¡œ ì ì ˆí•˜ê²Œ í•´ê²°ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê³¼ì í•©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´í›„ ì„¹ì…˜ì—ì„œ ì •ê·œí™”(íŠ¹íˆ ë“œë¡­ì•„ì›ƒ)ì˜ ë” ë§ì€ í˜•íƒœë¥¼ ë³´ê²Œ ë  ê²ƒì…ë‹ˆë‹¤.


# 5. Additional references
- https://cs231n.github.io
