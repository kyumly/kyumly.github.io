---
layout : single-3
title : "과제2. Fully-Connected-Nets"
categories : cs231
tag : [python, AI, assignment]
toc : true
toc_sticky : true
author_profile : false
use_math : true
---

<h1>다층 신경망 로직 구현</h1>

# 1. introduce
과제 1번에서는 쉽게 hidden-layer 한 개만 있을 경우 구현을 완료했습니다. 하지만 신경망은 여러개로 존재합니다.<br>
**심층 신경망을 구체적으로 구현하는 과정을 걸치게 됩니다.** <br><br>
![kyumly]({{site.url}}/images/assignment/two/full-img01.png){: .align-center .img-width-half}

# 2. init
네트워크를 구성하게 된다면 첫번째 신경망에서 할 일은 파라미터값을 정의 해야합니다.<br>
파라미터는 weight, bias 등 네트워크에 기본이 되는 값들을 설정해야 합니다.<br><br>

![kyumly]({{site.url}}/images/assignment/two/full-img02.png){: .align-center .img-width-half}


~~~python
        hidden_dim_layer = copy.deepcopy(hidden_dims)

        hidden_dim_layer.insert(0, input_dim)
        hidden_dim_layer.append(num_classes)
        for index in range(1,len(hidden_dim_layer)):
            self.params[f"W{index}"] = np.random.randn(hidden_dim_layer[index -1], hidden_dim_layer[index]) * weight_scale
            self.params[f"b{index}"] = np.zeros((1, hidden_dim_layer[index]))

~~~
<br>
<div>
out layer 기준으로 가중치, bias를 효율적으로 초기화를 해야합니다.<br>
input layer 기준으로 한다면, input 기준으로 한다면, bias 설정에서 for 문안에서 if 작성으로 분기를 나눠야한다.
</div>
<br>
![kyumly]({{site.url}}/images/assignment/two/full-img03.png){: .align-center .img-width-half}


# 3. train
~~~python
        cache_list = []
        input_list = [0, X]

        for index in range(1, self.num_layers +1):
            keys = [f'W{index}', f'b{index}', f'gamma{index}', f'beta{index}']
            w, b, gamma, beta = (self.params.get(k, None) for k in keys)  # get param vals
            bn = self.bn_params[index -1] if gamma is not None else None



            #fc, cache = affine_relu_forward(input_list[index], w, b)

            if index == self.num_layers:
                fc, cache = affine_forward(input_list[index], self.params[f'W{index}'], self.params[f'b{index}'])
            else:
                fc, cache = affine_batchnoral_relu_forward(input_list[index], w, b, gamma, beta, bn,
                                                           bn_type=self.normalization)
            input_list.append(fc)
            cache_list.append(cache)
~~~

![kyumly]({{site.url}}/images/assignment/two/full-img04.png){: .align-center .img-width-half}

[forward 하는 방식](https://kyumly.github.io/cs231/%EA%B3%BC%EC%A0%9C1.Two-Layer-Neural-Network/#3-train) 링크 클릭하면 이전에 설명했던 블로그 내용 나옵니다!

# 4. loss

~~~python
  loss, dz = softmax_loss(scores, y)
  weight = 0
  
  for index in range(self.num_layers, 0, -1):
      weight = (self.params[f'W{index}'] * self.params[f'W{index}']).sum()
      if index == self.num_layers:
          dz, grads[f'W{index}'], grads[f'b{index}'] = affine_backward(dz, cache_list[index - 1])
      else:
          if self.normalization == 'batchnorm' or self.normalization == 'layernorm':
              dz, grads[f'W{index}'], grads[f'b{index}'], grads[f'gamma{index}'], grads[
                  f'beta{index}'] = affine_batchnoral_relu_backward(dz, cache_list[index - 1], bn_type=self.normalization)
          else:
              dz, grads[f'W{index}'], grads[f'b{index}'] = affine_relu_backward(dz, cache_list[index - 1])
      grads[f'W{index}'] += self.reg * self.params[f'W{index}']
  
  loss += 0.5 * self.reg * weight
~~~
[backward 하는 방식](https://kyumly.github.io/cs231/%EA%B3%BC%EC%A0%9C1.Two-Layer-Neural-Network/#3-train) 링크 클릭하면 이전에 설명했던 블로그 내용 나옵니다!
