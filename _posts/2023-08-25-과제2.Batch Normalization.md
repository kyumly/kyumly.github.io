---
layout : single-3
title : "과제2. Batch Normalization"
categories : cs231
tag : [python, AI, assignment]
toc : true
toc_sticky : true
author_profile : false
use_math : true
---

<h1>배치 정규화 코드 구현</h1>

# 1. Introduce
신경망을 학습하는데 문제점을 흔하게 발생하는 문제점이 Vanishing, Exploding 발생한다.
잘 사용하지 않지만, 설명하기 좋은 Relu 함수를 가지고 설명을 한다면<br><br>

Relu 값은 음수로 값이 가게 된다면 모든 값이 0이 된다. 그 상태로 backpropagation 실행한다면,
\\(dw = X * \frac{dLoss}{dv}\\) X 값들은 모두 0이 될것이다. 그럼 역전파가 제대로 발생하지 않을 것이다.<br><br>

반대로 Exploding 값이 된다면 
\\(dw = X * \frac{dLoss}{dv}\\) X 큰값을 가지기 때문에 값이 폭팔하게 될것이다.<br><br>

이것을 해결하기 가중치를 적절하게 초기화 해야하지만, 결국 layer 지날수록 값들은 0으로 수렴하게 될것이다. (Xavier 초기화 또는 He 초기화 한다면 문제를 그나마 해결할 수 있음)
그래서 해결 방법으로 Batch Normalization 논문이 나오게 되었다.<br><br>


Batch Normalization 사용방법은 아래와 같은 계산법을 사용하게 된다.

$$
\begin{align}
& \mu=\frac{1}{N}\sum_{k=1}^N x_k  &  v=\frac{1}{N}\sum_{k=1}^N (x_k-\mu)^2 \\
& \sigma=\sqrt{v+\epsilon}         &  y_i=\frac{x_i-\mu}{\sigma} \\
& y_i = \gamma*y_i + \beta
\end{align}
$$
![kyumly]({{site.url}}/images/assignment/two/batch-img03.png){: .align-center .img-width-half-2}<br>

# 2. Batch Norm Forward
## 2.1 forward 과정
### 2.1.1 원리
1. 뉴런당 해당하는 평균 값을 구한다.
2. 분산을 구하고
3. 표준편차 =분산 + eps 구한다.
4. 정규분포를 구해
5. gamma, beta 추가해준다.

### 2.1.2 code
~~~python
def batchnorm_forward(x, gamma, beta, bn_param):

    mode = bn_param["mode"]
    eps = bn_param.get("eps", 1e-5)
    momentum = bn_param.get("momentum", 0.9)

    N, D = x.shape
    running_mean = bn_param.get("running_mean", np.zeros(D, dtype=x.dtype))
    running_var = bn_param.get("running_var", np.zeros(D, dtype=x.dtype))

    out, cache = None, None
    if mode == "train":

        x_mean = np.mean(x, axis=0)
        var = np.var(x, axis=0)

        std = np.sqrt(var + eps)
        x_hat = (x - x_mean) / std
        out = gamma * x_hat + beta

        shape = bn_param.get('shape', (N, D))
        axis = bn_param.get('axis', 0)

        cache = x, x_mean, var, std, gamma, x_hat, shape, axis # save for backprop

        if axis == 0:
            running_mean = momentum * running_mean + (1 - momentum) * x_mean # update overall mean
            running_var = momentum * running_var + (1 - momentum) * var  # update overall variance

    elif mode == "test":
        x_hat = (x - running_mean) / np.sqrt(running_var + eps)
        out = gamma * x_hat + beta

    else:
        raise ValueError('Invalid forward batchnorm mode "%s"' % mode)

    bn_param["running_mean"] = running_mean
    bn_param["running_var"] = running_var

    return out, cache
~~~
## 2.2 backward 과정
### 2.2.1 원리
<div class="div-content-center">
  <br>
  밑에 그림은 backpropagation 진행하는 과정을 표시했다.
  <br>
</div>
![kyumly]({{site.url}}/images/assignment/two/batch-img01.png){: .align-center .img-width-half-2}<br>
![kyumly]({{site.url}}/images/assignment/two/batch-img02.png){: .align-center .img-width-half-2}<br>

### 2.2.2 code
~~~python
def batchnorm_backward(dout, cache):
    x, mu, var, std, gamma, x_hat, shape, axis = cache

    dbeta = dout.sum(axis=0)
     
    dgamma = (dout * x_hat).sum(axis=0)

    dx_hat = dou t * gamma

    dstd = -np.sum(dx_hat * (x-mu), axis=0) / (std**2)

    dvar = 0.5 * dstd / std

    dx_1 = dx_hat / std

    dx_2 = 2 * (x - mu) * dvar / len(dout)

    dx_k1 = dx_1 + dx_2

    dmu = -np.sum(dx_k1, axis=0)  # derivative w.t.r. mu

    dx_k2 = dmu / len(dout)  # partial derivative w.t.r. dx

    dx = dx_k1 + dx_k2

    return dx, dgamma, dbeta
~~~

### 2.2.3 backnorm alt
![kyumly](https://raw.githubusercontent.com/cs231n/cs231n.github.io/master/assets/a2/batchnorm_graph.png)
![kyumly]({{site.url}}/images/assignment/two/batch-img04.png){: .align-center .img-width-half-2}<br>


~~~python
def batchnorm_backward_alt(dout, cache):
    dx, dgamma, dbeta = None, None, None

    _, _, _, std, gamma, x_hat, shape, axis = cache  # expand cache

    dbeta = dout.sum(axis=axis)
    dgamma = (dout * x_hat).sum(axis=axis)

    dx_hat = dout * gamma
    N = len(dout)

    dx = dx_hat / std

    dx = dx - (((dx * x_hat).sum(axis=0) * x_hat) / N) - (dx.sum(axis=0) / N)

    return dx, dgamma, dbeta
~~~

## Additional references
- https://cs231n.github.io



