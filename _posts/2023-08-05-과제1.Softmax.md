---
layout : single-3
title : "과제1. Implement a Softmax classifier"
categories : cs231
tag : [python, AI, assignment]
toc : true
toc_sticky : true
author_profile : false
use_math : true
---

# cs231n 과제1. Implement a Softmax classifier 정리

## 1.1 소개
알고리즘을 사용하면서 대표적인 분류 문제를 해결하기 위헤 사용하는 Loss Function Multiclass SVM loss, sotfmax classifier 
두가지로 분류가 된다.
이번시간에는 SVM 사용 방법에 대한 설명이다.

앞에서는 SVM 사용한 다중 분류를 사용했다면, 이번시간에는 딥러닝에서 많이 사용하는 분류 방법인 sotfmax classifier 구현할 것이다.
sotfmax classifier 대표적인 장점은 확률 값으로 score 점수를 나타내기 때문에 쉽게 해석이 가능하다.


$$
y_i = (\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} })
$$


그림은 softmax 함수이다. 모든 클래스 점수를 exp 시켜 모두 더하고 해당 클래스 점수를 분자로 표기해 해당 클래스의 확률을 구하게된다.
Softmax 총합은 1로 고정한다.<br>

$$
L_i = -\log\left(y_i\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -y_i + \log\sum_j {y_j}
$$

softmax 손실 함수는 \\(-\log(y_i)\\) 사용해 해당 클래스의 손실도를 구하게 된다.


$$
P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }
$$

<br>
만약 cross-entropy 함수를 사용하게 된다. Y target 값은 정답에만 1을 가지고 나머지 값들은 0을 가지게 된다
<br>

$$
H(p,q) = - \sum_x p(x) \log q(x)
$$

<br>
개인적인 생각으로 cross-entrpy 0을 곱하는 과정이나 위에 softmax loss \\(-\log(y_i)\\) 결론은 같다. 
<br>

$$
\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}
= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}
= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}
$$

<br>
스케일에 대한 문제가 있다. Numeric stabilty 관점에서는 큰값을 나누면 불안정하다
그래서 C는 score 최댓값을 구해 score에서 빼준다. 
<br>

![kyumly]({{site.url}}/images/assignment/one/softmax-img1.png){: .align-center .img-width-half}<br>



## 1.2 \\(L\\) + reg

$$
L =  \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\\\
$$

<br>
소개에서 봤던 \\(-\log(y_i)\\) 수식은 각 데이터들에 대한 손실도 였기 때문에 
전체 데이터 손실을 구해야한다.<br><br>

데이터 손실 부분을 먼저 구하고 \\(\underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss}\\)<br>

규제에 대한 손실값도 추가해야한다.\\(\underbrace{ \lambda R(W) }_\text{regularization loss}\\) <br>

규제 방식은 이전에 사용했던 L2 규제를 사용하기 때문에  모든 가중치에 제곱하고 더한다\\(\lambda \sum_k\sum_l W_{k,l}^2\\)


# 2. train
## 2.1 softmax loss naive
![kyumly]({{site.url}}/images/assignment/one/svm-img3.png){: .align-center .img-width-half}<br>
먼저 linear classifier \\(W * x + b\\) 사용해 class scores 설정해야한다.<br><br>
각각 파아미터는 \\(W\\), \\(x\\), \\(b\\) 파리미터롤 사용이 될것이다. 

![kyumly]({{site.url}}/images/assignment/one/svm-img4.png){: .align-center .img-width-half}<br>
최종 적인 class scores 받는 그림이다.<br>
이제 최적에 값을 찾아 가기위해서 Loss function 사용해 W 값을 업데이트 시킨다. <br><br>
W 값에 최적에 값을 찾아가는 방식은 경사하강법을 사용하게된다. (미분한다!)

~~~python
    cnt = 0
    for i in range(num_train):
        wx = X[i].dot(W)
        wx = wx - np.max(wx)
        score = np.exp(wx)
        # softmax 200 by 10
        softmax = score / np.sum(score)

        loss += -np.log(softmax[y[i]])

        for j in range(num_classes):
            dW[:, j] += softmax[j] * X[i]
        dW[:, y[i]] -= X[i]


    loss /= num_train
    dW /= num_train

    loss += reg * np.sum(W * W)
    dW += 2 * reg * W
~~~

- wx = X[i].dot(W) : scores 점수를 계산한다.
- wx = wx - np.max(wx) : score 점수에서 최댓값을 빼 정규화 시킨다.
- score = np.exp(wx) : 모든 값을 exp 지수화 시킨다.
- softmax = score / np.sum(score) : 행 데이터 기준으로 더하고 해당 정답 score 나누게 한다.
- loss += -np.log(softmax[y[i]]) : Loss 구한다.

![kyumly]({{site.url}}/images/assignment/one/softmax-img3.png){: .align-center .img-width-half-2}<br>

## 2.2 softmax dx naive
![kyumly]({{site.url}}/images/assignment/one/softmax-img4.png){: .align-center .img-width-half-2}<br>
![kyumly]({{site.url}}/images/assignment/one/softmax-img5.png){: .align-center .img-width-half-2}<br>



